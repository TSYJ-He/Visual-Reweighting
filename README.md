# Multi-Modal Token-level RL Reweighting ï¼ˆè¿˜æ²¡æƒ³å¥½åå„¿ï¼‰

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)


## ğŸ“‹ ç›®å½•

- [ä¸»è¦ç‰¹æ€§](#ä¸»è¦ç‰¹æ€§)
- [æ ¸å¿ƒç®—æ³•](#æ ¸å¿ƒç®—æ³•)
- [ç¯å¢ƒè¦æ±‚](#ç¯å¢ƒè¦æ±‚)
- [å®‰è£…](#å®‰è£…)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [è¯¦ç»†é…ç½®](#è¯¦ç»†é…ç½®)
- [è®­ç»ƒæµç¨‹](#è®­ç»ƒæµç¨‹)
- [é«˜çº§åŠŸèƒ½](#é«˜çº§åŠŸèƒ½)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
- [ç¤ºä¾‹](#ç¤ºä¾‹)
- [æ¶æ„è¯´æ˜](#æ¶æ„è¯´æ˜)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

## âœ¨ ä¸»è¦ç‰¹æ€§

- ğŸš€ **é«˜æ€§èƒ½åˆ†å¸ƒå¼è®­ç»ƒ**: åŸºäº Ray å’Œ FSDP çš„åˆ†å¸ƒå¼è®­ç»ƒæ¶æ„
- ğŸ¯ **Tokençº§ä¼˜åŒ–**: æ”¯æŒ Token Gradient Filtering (TGF) å’Œ Trajectory Advantage Shaping (TAS)
- ğŸ–¼ï¸ **å¤šæ¨¡æ€æ”¯æŒ**: åŸç”Ÿæ”¯æŒæ–‡æœ¬ã€å›¾åƒã€è§†é¢‘ç­‰å¤šæ¨¡æ€è¾“å…¥
- âš¡ **åŠ¨æ€æ‰¹å¤„ç†**: Padding-Free è®­ç»ƒå’ŒåŠ¨æ€æ‰¹å¤„ç†ï¼Œæ˜¾è‘—æå‡è®­ç»ƒæ•ˆç‡
- ğŸ”§ **çµæ´»é…ç½®**: åŸºäº OmegaConf çš„å±‚æ¬¡åŒ–é…ç½®ç³»ç»Ÿ
- ğŸ“Š **å®Œå–„ç›‘æ§**: æ”¯æŒ WandBã€TensorBoardã€MLflow ç­‰å¤šç§æ—¥å¿—ç³»ç»Ÿ
- ğŸ’¾ **æ–­ç‚¹ç»­è®­**: å®Œæ•´çš„æ£€æŸ¥ç‚¹ç®¡ç†å’Œè‡ªåŠ¨æ¢å¤æœºåˆ¶

## ğŸ§  æ ¸å¿ƒç®—æ³•

MTRL å®ç°äº†åˆ›æ–°çš„ **Token Perception Reinforcement Learning (TPRL)** ç®—æ³•ï¼ŒåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæŠ€æœ¯ï¼š

### Token Gradient Filtering (TGF) - å¾®è§‚çº§ä¼˜åŒ–
- **ç†µè¿‡æ»¤** (`enable_entropy_filtering`): åŸºäº token ç†µå€¼ç­›é€‰é«˜ä¸ç¡®å®šæ€§çš„ token
- **æ„ŸçŸ¥è¿‡æ»¤** (`enable_perception_filtering`): åŸºäºè§†è§‰ä¾èµ–åº¦ç­›é€‰è§†è§‰ç›¸å…³ token

### Trajectory Advantage Shaping (TAS) - å®è§‚çº§ä¼˜åŒ–
- **è½¨è¿¹é‡åŠ æƒ** (`enable_trajectory_reweighting`): åŸºäºè½¨è¿¹çš„è§†è§‰æ„ŸçŸ¥æ•æ„Ÿåº¦åŠ¨æ€è°ƒæ•´ä¼˜åŠ¿å‡½æ•°

## ğŸ“¦ ç¯å¢ƒè¦æ±‚

- Python >= 3.9
- CUDA >= 11.8 (æ¨è 12.1+)
- PyTorch >= 2.0
- å¤š GPU ç¯å¢ƒ (æ¨è 8 å¡æˆ–ä»¥ä¸Š)


## ğŸ”§ å®‰è£…





### 1. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

```bash
conda create -n mtrl python=3.10
conda activate mtrl
```

### 2. å®‰è£…ä¾èµ–

```bash
# å®‰è£… PyTorch (æ ¹æ® CUDA ç‰ˆæœ¬)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# å®‰è£… Flash Attention
pip install flash-attn --no-build-isolation

# å®‰è£… MTRL
pip install -e .



### 3. éªŒè¯å®‰è£…

```bash
python -c "import mtrl; print('MTRL å®‰è£…æˆåŠŸ!')"
```



### å‡†å¤‡æ•°æ®

æ•°æ®æ ¼å¼åº”ä¸º JSONLï¼Œæ¯è¡ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š

```json
{
    "prompt": "è¯·æè¿°è¿™å¼ å›¾ç‰‡",
    "answer": "è¿™æ˜¯ä¸€å¼ ...",
    "images": ["path/to/image1.jpg"],
    "videos": []
}
```

æ”¯æŒçš„å­—æ®µï¼š
- `prompt`: è¾“å…¥æç¤ºæ–‡æœ¬
- `answer`: å‚è€ƒç­”æ¡ˆï¼ˆå¯é€‰ï¼‰
- `images`: å›¾ç‰‡è·¯å¾„åˆ—è¡¨
- `videos`: è§†é¢‘è·¯å¾„åˆ—è¡¨

### åˆ›å»ºé…ç½®æ–‡ä»¶

åˆ›å»º `config.yaml`:

```yaml
# ==================== æ•°æ®é›†é…ç½® ====================
dataset:
  train_files: "data/train.jsonl"
  val_files: "data/val.jsonl"
  prompt_key: "prompt"
  answer_key: "answer"
  image_key: "images"
  video_key: "videos"
  image_dir: "data/images"
  max_prompt_length: 512
  max_response_length: 512
  rollout_batch_size: 256
  val_batch_size: 64

# ==================== è®­ç»ƒé…ç½® ====================
training:
  total_epochs: 10
  project_name: "mtrl_training"
  experiment_name: "qwen2_vl_7b"
  logger: ["console", "wandb"]
  nnodes: 1
  n_gpus_per_node: 8
  save_freq: 500
  val_freq: 100

# ==================== ä¼˜åŒ–é…ç½® ====================
optimization:
  # åŸºç¡€ä¼˜åŒ–å‚æ•°
  gamma: 1.0
  lam: 0.95
  advantage_estimator: "gae"
  
  # KL æ•£åº¦æ§åˆ¶
  divergence_coef: 0.01
  divergence_type: "adaptive"
  divergence_target: 6.0
  divergence_horizon: 10000.0
  
  # TPRL 
  enable_entropy_filtering: true       # å¯ç”¨ç†µè¿‡æ»¤ï¼ˆSARçš„é—äº§ï¼Œå…ˆæ”¾è¿™äº†ï¼‰
  top_p_entropy_tokens: 0.2           # é€‰æ‹© top 20% é«˜ç†µ token
  
  enable_perception_filtering: true    # å¯ç”¨æ„ŸçŸ¥è¿‡æ»¤
  top_p_perception_tokens: 0.2        # é€‰æ‹© top 20% è§†è§‰ç›¸å…³ token
  
  enable_trajectory_reweighting: true  # å¯ç”¨è½¨è¿¹é‡åŠ æƒ
  trajectory_scaling_min: 0.8         # æœ€å°ç¼©æ”¾å› å­

# ==================== Worker é…ç½® ====================
worker:
  hybrid_engine: true
  
  # Actor é…ç½®
  actor:
    model:
      model_path: "Qwen/Qwen2.5-VL-7B-Instruct" #qwen3å¥½ä¸€äº›
      enable_gradient_checkpointing: true
      trust_remote_code: true
    
    optim:
      lr: 5e-7
      weight_decay: 0.01
      warmup_steps: 10
    
    fsdp:
      sharding_strategy: "FULL_SHARD"
      mixed_precision_dtype: "bf16"
      cpu_offload: false
    
    # è®­ç»ƒå‚æ•°
    global_batch_size: 256
    micro_batch_size_per_device_for_update: 2
    ppo_epochs: 1
    clip_ratio_low: 0.2
    clip_ratio_high: 0.3
    max_grad_norm: 1.0
    padding_free: true
    dynamic_batching: true
  
  # Rollout é…ç½®
  rollout:
    name: "vllm"
    gpu_memory_utilization: 0.4
    temperature: 1.0
    top_p: 1.0
    top_k: -1
  
  # Reward é…ç½®
  reward:
    reward_type: "function"
    reward_fn_path: "your_reward_function.py"
```

### è¿è¡Œè®­ç»ƒ

```bash
# åŸºç¡€è®­ç»ƒ
python -m mtrl.training.training_main config=config.yaml

# ä½¿ç”¨å‘½ä»¤è¡Œè¦†ç›–é…ç½®
python -m mtrl.training.training_main \
    config=config.yaml \
    training.experiment_name=my_experiment \
    optimization.enable_entropy_filtering=true \
    optimization.enable_perception_filtering=true
```

## âš™ï¸ è¯¦ç»†é…ç½®

### æ•°æ®é›†é…ç½® (DatasetConfig)

```yaml
dataset:
  # æ•°æ®è·¯å¾„
  train_files: "data/train.jsonl"      # è®­ç»ƒæ•°æ®ï¼Œæ”¯æŒé€—å·åˆ†éš”å¤šä¸ªæ–‡ä»¶
  val_files: "data/val.jsonl"          # éªŒè¯æ•°æ®
  
  # æ•°æ®å­—æ®µå
  prompt_key: "prompt"                 # è¾“å…¥æç¤ºçš„å­—æ®µå
  answer_key: "answer"                 # ç­”æ¡ˆå­—æ®µå
  image_key: "images"                  # å›¾ç‰‡è·¯å¾„å­—æ®µå
  video_key: "videos"                  # è§†é¢‘è·¯å¾„å­—æ®µå
  
  # å¤šæ¨¡æ€é…ç½®
  image_dir: "data/images"             # å›¾ç‰‡ç›®å½•ï¼ˆå¦‚æœè·¯å¾„æ˜¯ç›¸å¯¹çš„ï¼‰
  video_fps: 2.0                       # è§†é¢‘é‡‡æ ·å¸§ç‡
  min_pixels: 262144                   # æœ€å°åƒç´ æ•° (512x512)
  max_pixels: 4194304                  # æœ€å¤§åƒç´ æ•° (2048x2048)
  
  # é•¿åº¦é…ç½®
  max_prompt_length: 512               # æœ€å¤§æç¤ºé•¿åº¦
  max_response_length: 512             # æœ€å¤§ç”Ÿæˆé•¿åº¦
  
  # æ‰¹å¤„ç†é…ç½®
  rollout_batch_size: 256              # ç”Ÿæˆæ‰¹æ¬¡å¤§å°
  val_batch_size: 64                   # éªŒè¯æ‰¹æ¬¡å¤§å°
  
  # æ•°æ®å¤„ç†
  shuffle: true                        # æ˜¯å¦æ‰“ä¹±æ•°æ®
  seed: 42                            # éšæœºç§å­
  filter_overlong_prompts: true       # è¿‡æ»¤è¿‡é•¿çš„æç¤º
```

### ä¼˜åŒ–é…ç½® (OptimizationConfig)

```yaml
optimization:
  # ============ åŸºç¡€ RL å‚æ•° ============
  gamma: 1.0                          # æŠ˜æ‰£å› å­
  lam: 0.95                          # GAE lambda å‚æ•°
  advantage_estimator: "gae"         # ä¼˜åŠ¿ä¼°è®¡å™¨: gae, grpo, rloo, remax
  
  # ============ KL æ•£åº¦æ§åˆ¶ ============
  disable_divergence: false          # æ˜¯å¦ç¦ç”¨å‚è€ƒæ¨¡å‹
  use_divergence_loss: false         # ä½¿ç”¨æ•£åº¦æŸå¤±è€Œéå¥–åŠ±æƒ©ç½š
  divergence_penalty: "kl"           # æ•£åº¦ç±»å‹: kl, abs, mse, low_var_kl
  divergence_coef: 0.01              # æ•£åº¦ç³»æ•°
  divergence_type: "adaptive"        # æ§åˆ¶å™¨: fixed, adaptive
  divergence_target: 6.0             # è‡ªé€‚åº”ç›®æ ‡æ•£åº¦
  divergence_horizon: 10000.0        # è‡ªé€‚åº”æ—¶é—´èŒƒå›´
  
  # ============ åœ¨çº¿è¿‡æ»¤ ============
  online_filtering: false            # å¯ç”¨åœ¨çº¿æ ·æœ¬è¿‡æ»¤
  filter_key: "overall"              # è¿‡æ»¤ä½¿ç”¨çš„å¥–åŠ±é”®
  filter_low: 0.01                   # è¿‡æ»¤ä½äºæ­¤åˆ†ä½æ•°çš„æ ·æœ¬
  filter_high: 0.99                  # è¿‡æ»¤é«˜äºæ­¤åˆ†ä½æ•°çš„æ ·æœ¬
  
  # ============ TPRL ç®—æ³• ============
  # Token Gradient Filtering - ç†µè¿‡æ»¤
  enable_entropy_filtering: true     # å¯ç”¨åŸºäºç†µçš„ token è¿‡æ»¤
  top_p_entropy_tokens: 0.2         # é€‰æ‹©é«˜ç†µ token çš„æ¯”ä¾‹
  
  # Token Gradient Filtering - æ„ŸçŸ¥è¿‡æ»¤
  enable_perception_filtering: true  # å¯ç”¨åŸºäºè§†è§‰æ„ŸçŸ¥çš„ token è¿‡æ»¤
  top_p_perception_tokens: 0.2      # é€‰æ‹©é«˜æ„ŸçŸ¥ token çš„æ¯”ä¾‹
  
  # Trajectory Advantage Shaping
  enable_trajectory_reweighting: true # å¯ç”¨è½¨è¿¹çº§é‡åŠ æƒ
  trajectory_scaling_min: 0.8        # æœ€å°ç¼©æ”¾å› å­ (0-1)
  
  # ç†µæƒ©ç½š
  use_entropy_penalty: false         # å¯ç”¨ç†µæ­£åˆ™åŒ–
  entropy_penalty_coef: 0.06         # ç†µæƒ©ç½šç³»æ•°
```

### æ¨¡å‹é…ç½® (ModelConfig)

```yaml
worker:
  actor:
    model:
      # æ¨¡å‹è·¯å¾„
      model_path: "Qwen/Qwen2-VL-7B-Instruct"
      
      # æ¨¡å‹è®¾ç½®
      trust_remote_code: true                    # ä¿¡ä»»è¿œç¨‹ä»£ç 
      enable_gradient_checkpointing: true        # æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰
      peft_type: null                           # PEFT ç±»å‹: lora, qlora, null
      
    optim:
      # ä¼˜åŒ–å™¨é…ç½®
      lr: 5e-7                                  # å­¦ä¹ ç‡
      weight_decay: 0.01                        # æƒé‡è¡°å‡
      warmup_steps: 10                          # é¢„çƒ­æ­¥æ•°
      lr_scheduler_type: "constant_with_warmup" # å­¦ä¹ ç‡è°ƒåº¦å™¨
      betas: [0.9, 0.95]                       # Adam beta å‚æ•°
    
    fsdp:
      # FSDP é…ç½®
      sharding_strategy: "FULL_SHARD"           # åˆ†ç‰‡ç­–ç•¥
      mixed_precision_dtype: "bf16"             # æ··åˆç²¾åº¦: bf16, fp16
      cpu_offload: false                        # CPU offload
    
    # è®­ç»ƒè¶…å‚æ•°
    global_batch_size: 256                      # å…¨å±€æ‰¹æ¬¡å¤§å°
    micro_batch_size_per_device_for_update: 2   # æ¯å¡æ›´æ–°æ‰¹æ¬¡
    micro_batch_size_per_device_for_experience: 8  # æ¯å¡æ¨ç†æ‰¹æ¬¡
    ppo_epochs: 1                               # PPO æ›´æ–°è½®æ•°
    
    # PPO è£å‰ª
    clip_ratio_low: 0.2                         # ä¸‹ç•Œè£å‰ªæ¯”ç‡
    clip_ratio_high: 0.3                        # ä¸Šç•Œè£å‰ªæ¯”ç‡
    clip_ratio_dual: 3.0                        # åŒé‡è£å‰ªå¸¸æ•°
    
    # å…¶ä»–
    max_grad_norm: 1.0                          # æ¢¯åº¦è£å‰ª
    padding_free: true                          # Padding-free è®­ç»ƒ
    dynamic_batching: true                      # åŠ¨æ€æ‰¹å¤„ç†
    ulysses_size: 1                            # Ulysses åºåˆ—å¹¶è¡Œå¤§å°
    use_torch_compile: true                     # ä½¿ç”¨ torch.compile
```

### Rollout é…ç½® (RolloutConfig)

```yaml
worker:
  rollout:
    name: "vllm"                        # æ¨ç†å¼•æ“: vllm
    gpu_memory_utilization: 0.4         # GPU æ˜¾å­˜åˆ©ç”¨ç‡
    
    # ç”Ÿæˆå‚æ•°
    temperature: 1.0                    # é‡‡æ ·æ¸©åº¦
    top_p: 1.0                         # nucleus é‡‡æ ·
    top_k: -1                          # top-k é‡‡æ · (-1 ç¦ç”¨)
    max_new_tokens: 512                # æœ€å¤§ç”Ÿæˆé•¿åº¦
    
    # vLLM ç‰¹å®šé…ç½®
    tensor_parallel_size: 1            # å¼ é‡å¹¶è¡Œå¤§å°
    enable_prefix_caching: false       # å‰ç¼€ç¼“å­˜
```

### Reward é…ç½® (RewardConfig)

```yaml
worker:
  reward:
    reward_type: "function"             # å¥–åŠ±ç±»å‹: function, model
    reward_fn_path: "path/to/reward.py" # å¥–åŠ±å‡½æ•°è·¯å¾„
    num_cpus: 4                        # CPU æ ¸å¿ƒæ•°
```

å¥–åŠ±å‡½æ•°ç¤ºä¾‹ (`reward.py`):

```python
def reward_function(data_dict):
    """
    è®¡ç®—å¥–åŠ±å‡½æ•°
    
    Args:
        data_dict: åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸
            - prompt: è¾“å…¥æç¤º
            - response: æ¨¡å‹ç”Ÿæˆçš„å“åº”
            - ground_truth: å‚è€ƒç­”æ¡ˆï¼ˆå¦‚æœæœ‰ï¼‰
            - images: å›¾ç‰‡æ•°æ®
    
    Returns:
        reward: floatï¼Œå¥–åŠ±å€¼
        metrics: dictï¼Œé¢å¤–çš„æŒ‡æ ‡
    """
    prompt = data_dict['prompt']
    response = data_dict['response']
    ground_truth = data_dict.get('ground_truth', '')
    
    # ç¤ºä¾‹ï¼šåŸºäºé•¿åº¦å’Œå…³é”®è¯çš„ç®€å•å¥–åŠ±
    reward = 0.0
    
    # é•¿åº¦å¥–åŠ±
    if 50 <= len(response) <= 200:
        reward += 0.5
    
    # å…³é”®è¯åŒ¹é…
    if ground_truth:
        keywords = set(ground_truth.split())
        response_words = set(response.split())
        overlap = len(keywords & response_words) / len(keywords)
        reward += overlap
    
    metrics = {
        "length": len(response),
        "overlap": overlap if ground_truth else 0
    }
    
    return reward, metrics
```

## ğŸ“Š è®­ç»ƒæµç¨‹

### å®Œæ•´è®­ç»ƒæµç¨‹

```python
# 1. æ•°æ®å‡†å¤‡
# 2. é…ç½®æ–‡ä»¶ç¼–å†™
# 3. å¯åŠ¨è®­ç»ƒ
python -m mtrl.training.training_main config=config.yaml

# 4. ç›‘æ§è®­ç»ƒ
# - æŸ¥çœ‹ WandB é¢æ¿
# - æ£€æŸ¥æ—¥å¿—æ–‡ä»¶
# - ç›‘æ§ GPU ä½¿ç”¨ç‡

# 5. è¯„ä¼°å’Œéƒ¨ç½²
# - åŠ è½½æ£€æŸ¥ç‚¹
# - è¿è¡Œè¯„ä¼°è„šæœ¬
# - å¯¼å‡ºæœ€ç»ˆæ¨¡å‹
```

### è®­ç»ƒç›‘æ§

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šè¾“å‡ºä»¥ä¸‹å…³é”®æŒ‡æ ‡ï¼š

- **è®­ç»ƒæŒ‡æ ‡**:
  - `actor/pg_loss`: ç­–ç•¥æ¢¯åº¦æŸå¤±
  - `actor/approx_kl`: è¿‘ä¼¼ KL æ•£åº¦
  - `actor/clipfrac`: è£å‰ªæ¯”ä¾‹
  - `actor/entropy`: ç­–ç•¥ç†µ
  - `actor/grad_norm`: æ¢¯åº¦èŒƒæ•°

- **TPRL ç‰¹å®šæŒ‡æ ‡**:
  - `actor/entropy_token_fraction`: ç†µè¿‡æ»¤é€‰æ‹©çš„ token æ¯”ä¾‹
  - `actor/perception_token_fraction`: æ„ŸçŸ¥è¿‡æ»¤é€‰æ‹©çš„ token æ¯”ä¾‹
  - `actor/sensitivity_score_mean`: å¹³å‡æ•æ„Ÿåº¦åˆ†æ•°
  - `actor/scaling_factor_mean`: å¹³å‡ç¼©æ”¾å› å­

- **æ€§èƒ½æŒ‡æ ‡**:
  - `perf/tokens_per_second`: æ¯ç§’å¤„ç† token æ•°
  - `perf/samples_per_second`: æ¯ç§’æ ·æœ¬æ•°
  - `perf/throughput`: ååé‡

- **å¥–åŠ±æŒ‡æ ‡**:
  - `reward/overall`: æ€»ä½“å¥–åŠ±
  - `val/reward_score`: éªŒè¯é›†å¥–åŠ±

## ğŸ¯ é«˜çº§åŠŸèƒ½

### 1. ä½¿ç”¨ TPRL ç®—æ³•

å®Œæ•´å¯ç”¨ TPRLï¼ˆToken Perception RLï¼‰:

```yaml
optimization:
  # å¯ç”¨æ‰€æœ‰ TPRL ç»„ä»¶
  enable_entropy_filtering: true
  top_p_entropy_tokens: 0.2
  
  enable_perception_filtering: true
  top_p_perception_tokens: 0.2
  
  enable_trajectory_reweighting: true
  trajectory_scaling_min: 0.8
```

**å·¥ä½œåŸç†**:

1. **ç†µè¿‡æ»¤**: è¯†åˆ«æ¨¡å‹ä¸ç¡®å®šçš„ tokenï¼Œé‡ç‚¹ä¼˜åŒ–è¿™äº›ä½ç½®
2. **æ„ŸçŸ¥è¿‡æ»¤**: è¯†åˆ«ä¾èµ–è§†è§‰ä¿¡æ¯çš„ tokenï¼Œå¼ºåŒ–è§†è§‰ç†è§£
3. **è½¨è¿¹é‡åŠ æƒ**: æ ¹æ®æ•´ä½“è§†è§‰æ•æ„Ÿåº¦è°ƒæ•´è½¨è¿¹æƒé‡

### 2. åˆ†å¸ƒå¼è®­ç»ƒ

#### å•èŠ‚ç‚¹å¤šå¡

```bash
python -m mtrl.training.training_main \
    config=config.yaml \
    training.nnodes=1 \
    training.n_gpus_per_node=8
```

#### å¤šèŠ‚ç‚¹è®­ç»ƒ

```bash
# åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šè¿è¡Œ
# èŠ‚ç‚¹ 0 (master)
RAY_ADDRESS='auto' python -m mtrl.training.training_main config=config.yaml

# èŠ‚ç‚¹ 1, 2, ... (workers)
RAY_ADDRESS='ip-of-node-0:6379' ray start --address='ip-of-node-0:6379'
```

### 3. æ–­ç‚¹ç»­è®­

```yaml
training:
  load_checkpoint_path: "checkpoints/experiment/step_1000"
  find_last_checkpoint: true  # è‡ªåŠ¨æ‰¾åˆ°æœ€æ–°æ£€æŸ¥ç‚¹
```

æˆ–ä½¿ç”¨å‘½ä»¤è¡Œ:

```bash
python -m mtrl.training.training_main \
    config=config.yaml \
    training.load_checkpoint_path=checkpoints/experiment/step_1000
```

### 4. ä»…éªŒè¯æ¨¡å¼

```bash
python -m mtrl.training.training_main \
    config=config.yaml \
    training.val_only=true \
    training.load_checkpoint_path=checkpoints/best_model
```

### 5. ä½¿ç”¨ LoRA/QLoRA

```yaml
worker:
  actor:
    model:
      peft_type: "lora"
      lora_rank: 8
      lora_alpha: 16
      lora_dropout: 0.05
      lora_target_modules: ["q_proj", "v_proj"]
```

## âš¡ æ€§èƒ½ä¼˜åŒ–

### æ˜¾å­˜ä¼˜åŒ–

```yaml
worker:
  actor:
    model:
      enable_gradient_checkpointing: true  # æ¢¯åº¦æ£€æŸ¥ç‚¹
    
    fsdp:
      cpu_offload: true                   # CPU offloadï¼ˆæ…¢ä½†çœæ˜¾å­˜ï¼‰
      sharding_strategy: "FULL_SHARD"     # å…¨åˆ†ç‰‡
    
    offload:
      offload_params: true                # å‚æ•° offload
      offload_optimizer: true             # ä¼˜åŒ–å™¨ offload
    
    # å‡å°æ‰¹æ¬¡å¤§å°
    micro_batch_size_per_device_for_update: 1
    
  rollout:
    gpu_memory_utilization: 0.3          # é™ä½ vLLM æ˜¾å­˜å ç”¨
```

### é€Ÿåº¦ä¼˜åŒ–

```yaml
worker:
  actor:
    # å¢å¤§æ‰¹æ¬¡å¤§å°
    global_batch_size: 512
    micro_batch_size_per_device_for_update: 4
    
    # æ€§èƒ½ä¼˜åŒ–é€‰é¡¹
    padding_free: true                   # Padding-free è®­ç»ƒ
    dynamic_batching: true               # åŠ¨æ€æ‰¹å¤„ç†
    use_torch_compile: true              # Torch compile
    ulysses_size: 2                      # Ulysses åºåˆ—å¹¶è¡Œ
    
    fsdp:
      mixed_precision_dtype: "bf16"      # ä½¿ç”¨ BF16
      
  rollout:
    gpu_memory_utilization: 0.6          # æé«˜ vLLM åˆ©ç”¨ç‡
    enable_prefix_caching: true          # å¯ç”¨å‰ç¼€ç¼“å­˜
```

### æ··åˆç²¾åº¦è®­ç»ƒ

```yaml
worker:
  actor:
    fsdp:
      mixed_precision_dtype: "bf16"  # æ¨è BF16ï¼ˆA100/H100ï¼‰
      # mixed_precision_dtype: "fp16"  # V100 ä½¿ç”¨ FP16
```

## ğŸ“ ç¤ºä¾‹

### ç¤ºä¾‹ 1: åŸºç¡€æ–‡æœ¬ RL è®­ç»ƒ

```yaml
dataset:
  train_files: "data/train.jsonl"
  max_prompt_length: 512
  max_response_length: 256

optimization:
  advantage_estimator: "gae"
  gamma: 0.99
  lam: 0.95

worker:
  actor:
    model:
      model_path: "meta-llama/Llama-2-7b-hf"
    global_batch_size: 256
```

### ç¤ºä¾‹ 2: å¤šæ¨¡æ€ VQA è®­ç»ƒ

```yaml
dataset:
  train_files: "data/vqa_train.jsonl"
  image_dir: "data/images"
  image_key: "image_path"
  max_prompt_length: 512
  max_response_length: 128

optimization:
  enable_entropy_filtering: true
  enable_perception_filtering: true
  enable_trajectory_reweighting: true

worker:
  actor:
    model:
      model_path: "Qwen/Qwen2-VL-7B-Instruct"
```

### ç¤ºä¾‹ 3: æ•°å­¦æ¨ç†è®­ç»ƒ

```yaml
dataset:
  train_files: "data/math_train.jsonl"
  max_response_length: 1024

optimization:
  advantage_estimator: "grpo"
  enable_entropy_filtering: true
  top_p_entropy_tokens: 0.3  # æ•°å­¦æ¨ç†éœ€è¦æ›´å¤šæ¢ç´¢

worker:
  actor:
    model:
      model_path: "deepseek-ai/deepseek-math-7b"
    ppo_epochs: 2  # æ•°å­¦ä»»åŠ¡å¯ä»¥å¤šè½®æ›´æ–°
```

## ğŸ—ï¸ æ¶æ„è¯´æ˜

### æ¨¡å—ç»“æ„

```
mtrl/
â”œâ”€â”€ agents/                 # ç­–ç•¥ä»£ç†
â”‚   â”œâ”€â”€ base.py            # åŸºç¡€ä»£ç†æ¥å£
â”‚   â”œâ”€â”€ parallel_policy_agent.py  # åˆ†å¸ƒå¼ç­–ç•¥ä»£ç†
â”‚   â””â”€â”€ config.py          # ä»£ç†é…ç½®
â”œâ”€â”€ optimization/          # ä¼˜åŒ–ç®—æ³•
â”‚   â”œâ”€â”€ policy_optimization.py  # ç­–ç•¥ä¼˜åŒ–å‡½æ•°
â”‚   â””â”€â”€ config.py          # ä¼˜åŒ–é…ç½®
â”œâ”€â”€ single_controller/     # åˆ†å¸ƒå¼æ§åˆ¶
â”‚   â”œâ”€â”€ base/             # åŸºç¡€æ§åˆ¶å™¨
â”‚   â””â”€â”€ ray/              # Ray å®ç°
â”œâ”€â”€ training/             # è®­ç»ƒæµç¨‹
â”‚   â”œâ”€â”€ distributed_trainer.py  # åˆ†å¸ƒå¼è®­ç»ƒå™¨
â”‚   â”œâ”€â”€ training_main.py   # è®­ç»ƒå…¥å£
â”‚   â””â”€â”€ training_config.py # è®­ç»ƒé…ç½®
â”œâ”€â”€ workers/              # å·¥ä½œè¿›ç¨‹
â”‚   â”œâ”€â”€ actor/            # Actor worker
â”‚   â”œâ”€â”€ critic/           # Critic worker
â”‚   â”œâ”€â”€ rollout/          # Rollout worker
â”‚   â””â”€â”€ reward/           # Reward worker
â”œâ”€â”€ models/               # æ¨¡å‹é€‚é…
â”œâ”€â”€ utils/                # å·¥å…·å‡½æ•°
â””â”€â”€ protocol.py           # æ•°æ®åè®®
```

### è®­ç»ƒæµç¨‹å›¾

```
1. æ•°æ®åŠ è½½ â†’ 2. Rollout (ç”Ÿæˆ) â†’ 3. Reward è®¡ç®— 
                                          â†“
6. æ›´æ–°æ¨¡å‹ â† 5. PPO æ›´æ–° â† 4. Advantage è®¡ç®—
     â†“
7. é‡å¤æ­¥éª¤ 2-6
```

### TPRL ç®—æ³•æµç¨‹

```
è¾“å…¥: å¤šæ¨¡æ€æ•°æ® (æ–‡æœ¬ + å›¾åƒ)
  â†“
ç”Ÿæˆå“åº”
  â†“
è®¡ç®— log_probs å’Œ aug_log_probs
  â†“
Token-level è¿‡æ»¤:
  â”œâ”€ ç†µè¿‡æ»¤ â†’ é€‰æ‹©é«˜ä¸ç¡®å®šæ€§ token
  â””â”€ æ„ŸçŸ¥è¿‡æ»¤ â†’ é€‰æ‹©è§†è§‰ç›¸å…³ token
  â†“
Trajectory-level é‡åŠ æƒ:
  â””â”€ åŸºäºæ•æ„Ÿåº¦è°ƒæ•´ advantage
  â†“
PPO æ›´æ–°
```



 æ¨èçš„èµ·å§‹å€¼ï¼š

```yaml
optimization:
  enable_entropy_filtering: true
  top_p_entropy_tokens: 0.2          # èµ·å§‹å€¼ï¼Œå¯è°ƒæ•´ 0.1-0.3
  
  enable_perception_filtering: true
  top_p_perception_tokens: 0.2       # èµ·å§‹å€¼ï¼Œå¯è°ƒæ•´ 0.1-0.3
  
  enable_trajectory_reweighting: true
  trajectory_scaling_min: 0.8        # èµ·å§‹å€¼ï¼Œå¯è°ƒæ•´ 0.6-0.9
```

- å¦‚æœæ¨¡å‹è¿‡åº¦å…³æ³¨æŸäº› tokenï¼Œé™ä½ `top_p` å€¼
- å¦‚æœå¸Œæœ›æ›´å¼ºçš„è§†è§‰ç†è§£ï¼Œå¢åŠ  `top_p_perception_tokens`
- å¦‚æœè®­ç»ƒä¸ç¨³å®šï¼Œå¢åŠ  `trajectory_scaling_min`

### å¦‚ä½•åŠ é€Ÿè®­ç»ƒï¼Ÿ

**A**: æ€§èƒ½ä¼˜åŒ–å»ºè®®ï¼š

1. **å¯ç”¨æ‰€æœ‰åŠ é€Ÿé€‰é¡¹**:
```yaml
worker:
  actor:
    padding_free: true
    dynamic_batching: true
    use_torch_compile: true
```

2. **ä½¿ç”¨æ··åˆç²¾åº¦**: `mixed_precision_dtype: "bf16"`

3. **ä¼˜åŒ– vLLM é…ç½®**:
```yaml
worker:
  rollout:
    gpu_memory_utilization: 0.5
    enable_prefix_caching: true
```

4. **å¢å¤§æ‰¹æ¬¡å¤§å°**ï¼ˆå¦‚æœæ˜¾å­˜å…è®¸ï¼‰

###  è®­ç»ƒä¸­æ–­æ¢å¤

**A**: MTRL æ”¯æŒè‡ªåŠ¨æ–­ç‚¹ç»­è®­ï¼š

```yaml
training:
  find_last_checkpoint: true  # è‡ªåŠ¨å¯»æ‰¾æœ€æ–°æ£€æŸ¥ç‚¹
  load_checkpoint_path: "checkpoints/project/experiment"
```

æˆ–æ‰‹åŠ¨æŒ‡å®šï¼š

```bash
python -m mtrl.training.training_main \
    config=config.yaml \
    training.load_checkpoint_path=checkpoints/experiment/step_5000
```

### Q5: å¦‚ä½•è°ƒè¯•å¥–åŠ±å‡½æ•°ï¼Ÿ

**A**: ä½¿ç”¨éªŒè¯æ¨¡å¼å¿«é€Ÿæµ‹è¯•ï¼š

```bash
python -m mtrl.training.training_main \
    config=config.yaml \
    training.val_only=true \
    training.val_before_train=true \
    dataset.val_batch_size=10
```

### Q6: å¤šæ¨¡æ€è®­ç»ƒæ³¨æ„äº‹é¡¹ï¼Ÿ

**A**: 

1. **ç¡®ä¿å›¾ç‰‡è·¯å¾„æ­£ç¡®**:
```yaml
dataset:
  image_dir: "/absolute/path/to/images"  # ä½¿ç”¨ç»å¯¹è·¯å¾„
```

2. **è°ƒæ•´å›¾ç‰‡åˆ†è¾¨ç‡**:
```yaml
dataset:
  min_pixels: 262144   # 512x512
  max_pixels: 1048576  # 1024x1024ï¼ˆé™ä½ä»¥èŠ‚çœæ˜¾å­˜ï¼‰
```

3. **å¯¹äºè§†è§‰å¯†é›†ä»»åŠ¡ï¼Œå¢å¼ºæ„ŸçŸ¥è¿‡æ»¤**:
```yaml
optimization:
  enable_perception_filtering: true
  top_p_perception_tokens: 0.3  # å¢åŠ åˆ° 30%
```

### Q7: å¦‚ä½•ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹ï¼Ÿ

**A**: åªéœ€æŒ‡å®š HuggingFace æ¨¡å‹è·¯å¾„ï¼š

```yaml
worker:
  actor:
    model:
      model_path: "your-org/your-model"
      trust_remote_code: true  # å¦‚æœéœ€è¦
```

å¯¹äºæœ¬åœ°æ¨¡å‹ï¼š

```yaml
worker:
  actor:
    model:
      model_path: "/path/to/local/model"
```

### Q8: æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ

**A**: å¤šç§æ˜¾å­˜ä¼˜åŒ–ç­–ç•¥ï¼š

1. **å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹**:
```yaml
worker:
  actor:
    model:
      enable_gradient_checkpointing: true
```

2. **å¯ç”¨ CPU offload**:
```yaml
worker:
  actor:
    fsdp:
      cpu_offload: true
    offload:
      offload_params: true
      offload_optimizer: true
```

3. **å‡å°æ‰¹æ¬¡å¤§å°**:
```yaml
worker:
  actor:
    micro_batch_size_per_device_for_update: 1
```

4. **é™ä½ vLLM æ˜¾å­˜å ç”¨**:
```yaml
worker:
  rollout:
    gpu_memory_utilization: 0.3
```

5. **ä½¿ç”¨ LoRA**:
```yaml
worker:
  actor:
    model:
      peft_type: "lora"
```

## ğŸ™ è‡´è°¢

æœ¬é¡¹ç›®åŸºäºä»¥ä¸‹ä¼˜ç§€å¼€æºé¡¹ç›®ï¼š

- [Transformers](https://github.com/huggingface/transformers)
- [vLLM](https://github.com/vllm-project/vllm)
- [Ray](https://github.com/ray-project/ray)
- [PyTorch FSDP](https://pytorch.org/docs/stable/fsdp.html)


